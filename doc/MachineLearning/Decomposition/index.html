<!DOCTYPE html>
<html class="no-js" lang="en" >
  <head>
    <link rel="shortcut icon" href="https://iwasakishuto.github.io/images/apple-touch-icon/Kerasy.png" />

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="author" content="Shuto" />
    <meta name="twitter:card" content="summary">
    <meta property="og:url" content="https://iwasakishuto.github.io/Kerasy/doc/MachineLearning/Decomposition/index.html" />
    <meta property="og:title" content="Decomposition">
    <meta property="og:description" content="Brief explanation of Decomposition and introduction of modules that can be used in Kerasy">
    <meta property="og:image" content="https://iwasakishuto.github.io/images/FacebookImage/Kerasy.png"/>
    <meta property="og:type" content="article" />

    <title>Decomposition - Kerasy Documentation</title>
    <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

    <link rel="stylesheet" type="text/css" href="https://iwasakishuto.github.io/Kerasy/doc/theme/css/custom.css">
    <link rel="stylesheet" type="text/css" href="https://iwasakishuto.github.io/Kerasy/doc/theme/css/jupyter.css">
    <link rel="stylesheet" type="text/css" href="https://iwasakishuto.github.io/Kerasy/doc/theme/css/theme_extra.css">
    <link rel="stylesheet" type="text/css" href="https://iwasakishuto.github.io/Kerasy/doc/theme/css/theme.css">

    <script>
      // Current page data
      var mkdocs_page_name = "Decomposition";
      var mkdocs_page_input_path = "MachineLearning/Decomposition.md";
      var mkdocs_page_url = "/Kerasy/MachineLearning/Decomposition/";
    </script>
    <script src="https://iwasakishuto.github.io/Kerasy/doc/theme/js/jquery-2.1.1.min.js" defer></script>
    <script src="https://iwasakishuto.github.io/Kerasy/doc/theme/js/modernizr-2.8.3.min.js" defer></script>
    <script src="https://iwasakishuto.github.io/Kerasy/doc/theme/js/theme.js" defer></script>

    <!-- Common CSS in my portofolio  -->
    <link rel="stylesheet" type="text/css" href="https://iwasakishuto.github.io/css/jupyter.css">
    <link rel="stylesheet" type="text/css" href="https://iwasakishuto.github.io/css/custom.css" media="screen">
    <link rel="apple-touch-icon" sizes="152x152" href="https://iwasakishuto.github.io/images/apple-touch-icon/Kerasy.png" type="image/png" />
    <!-- Use fontawesome Icon -->
    <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.1/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href="https://use.fontawesome.com/releases/v5.7.0/css/all.css" rel="stylesheet" integrity="sha384-lZN37f5QGtY3VHgisS14W3ExzMWZxybE1SJSEsQp9S+oqd12jhcu+A56Ebc1zFSJ" crossorigin="anonymous">
    <!-- Syntax highlight -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.6/styles/github.min.css">
    <!-- Custom CSS -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/8.6/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML"></script>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
        }
      });
    </script>
    <!-- Mermaid -->
    <script src="https://unpkg.com/mermaid/dist/mermaid.min.js" charset="UTF-8"></script>
    <script>
      mermaid.initialize({
        startOnLoad:true
      });
    </script>
    <script src="https://iwasakishuto.github.io/js/smooth-scroll.polyfills.min.js"></script>
  </head>
<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href="../.." class="icon icon-home"> Kerasy Documentation</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="tocbase current">
    
    
      


  <li class="navtree toctree-l1 inactive">
    <a class="" href="../..">Home</a>
  </li>
    
      
  <li class="navtree toctree-l1 label">
    <p class="caption">DeepLearning</p>
  </li>


  

  
    <li class="navtree toctree-l1 group">
      <ul class="navtree subnav-l1 current">
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../DeepLearning/CNN/">CNN</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../DeepLearning/Optimizers/">Optimizers</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../DeepLearning/NeuralNetwork/">Neural Network</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../DeepLearning/ComputationalGraph/">Computational Graph</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../DeepLearning/Initializers/">Initializers</a>
  </li>
        
      </ul>
    </li>
    
      
  <li class="navtree toctree-l1 label">
    <p class="caption">MachineLearning</p>
  </li>


  

  
    <li class="navtree toctree-l1 group">
      <ul class="navtree subnav-l1 current">
          


  
    
    <li class="navtree toctree-l2 page current">
      <a class="current" href="./">
        Decomposition
          <span class="toctree-expand"></span>
      </a>
    </li>
    

  
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../sampling/">Sampling</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../Linear Regression/">Linear Regression</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../Cluster/">Cluster</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../HMM/">HMM</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../EM algorithm/">EM algorithm</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../Tree/">Tree</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../Support Vector Machine/">Support Vector Machine</a>
  </li>
        
      </ul>
    </li>
    
      
  <li class="navtree toctree-l1 label">
    <p class="caption">BioInformatics</p>
  </li>


  

  
    <li class="navtree toctree-l1 group">
      <ul class="navtree subnav-l1 current">
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../BioInformatics/Tandem Repeats/">Tandem Repeats</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../BioInformatics/RefSeq/">RefSeq</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../BioInformatics/Secondary Structure/">Secondary Structure</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../BioInformatics/Alignment/">Alignment</a>
  </li>
        
          


  <li class="navtree toctree-l2 inactive">
    <a class="" href="../../BioInformatics/String Search/">String Search</a>
  </li>
        
      </ul>
    </li>
    
  </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../..">Kerasy Documentation</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../..">Docs</a> &raquo;</li>
    
      
        
          <li>MachineLearning &raquo;</li>
        
      
    
    <li>Decomposition</li>
    <li class="wy-breadcrumbs-aside">
      
        <a href="https://github.com/iwasakishuto/Kerasy"> Visit github &nbsp; <i class="fab fa-github"></i></a>
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <div id="contents">
<div class="row">
    <div class="small-12 columns article">
        <div class="topRibbon">
          <h3>Decomposition</h3>
        </div>
        <div class="row info-bar" style="margin-left:0rem;margin-bottom:6px;">
    <div class="small-12 columns">
        <ul class="inline-list">
            <li><span><i class="fa fa-calendar"></i>&nbsp; 2019-11-26(火)</span></li>
            <li><span><i class="far fa-file-code"></i>&nbsp; <a href="https://github.com/iwasakishuto/Kerasy/blob/gh-pages/kerasy/ML/decomposition.py">ML/decomposition.py</a></span></li>
            <!-- <li><span><i class="fa fa-folder-open"></i>&nbsp; <a href="https://iwasakishuto.github.io/Kerasy/doc/category/machinelearning.html">MachineLearning</a></span></li> -->
        </ul>
    </div>
</div>
        <section class="article">
            <div class="frame">
  <h4>Purpose</h4>
  <p>The purpose of <b>decomposition (dimensionality reduction, feature extraction)</b> is to find the <font color="red"><b>"best"</b></font> <b>principal subspace</b> where data are expressed with much less dimensions, but keep the information as much as possible.</p>
</div>

<div class="admonition tip">
  <p class="admonition-title">Notebook</p>
  <p>Example Notebook: <a href="https://nbviewer.jupyter.org/github/iwasakishuto/Kerasy/blob/gh-pages/examples/decomposition.ipynb">Kerasy.examples.decomposition.ipynb</a></p>
</div>

<h2>PCA</h2>
<div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">kerasy</span><span class="o">.</span><span class="n">ML</span><span class="o">.</span><span class="n">decomposition</span><span class="o">.</span><span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</pre></div>


<p>Consider a dataset <span class="math">\(\{\mathbf{x}_n\}\)</span>. If we consider a <span class="math">\(D\)</span>-dimensional vector <span class="math">\(\mathbf{u}_1\)</span>, each data point <span class="math">\(\mathbf{x}_n\in\mathbb{R}^{D}\)</span> is projected onto a scalar value <span class="math">\(\mathbf{u}_1^T\mathbf{x}_n\)</span>. In that (one-dimensional) space,</p>
<ul>
<li>Mean:
<div class="math">$$\tilde{\mathbf{x}} = \frac{1}{N}\sum_{n=1}^N\mathbf{x}_n$$</div>
</li>
<li>Variance:
<div class="math">$$\frac{1}{N}\sum_{n=1}^N\left\{\mathbf{u}_1^T\mathbf{x}_n - \mathbf{u}_1^T\tilde{\mathbf{x}}\right\}^2 = \mathbf{u}_1^T\mathbf{Su}_1\quad\mathbf{S}= \frac{1}{N}\sum_{n=1}^N\left(\mathbf{x}-\tilde{\mathbf{x}}\right)\left(\mathbf{x}-\tilde{\mathbf{x}}\right)^T$$</div>
</li>
</ul>
<p>We now maximize the projected variance <span class="math">\(\mathbf{u}_1^T\mathbf{Su}_1\quad\mathbf{S}\)</span> with respect to <span class="math">\(\mathbf{u}_1\)</span>, because we can think <font color="red"><b>"Larger variance" ↔︎ "Features are more expressed"</b></font>.</p>
<p>To avoid <span class="math">\(\|\mathbf{u}_1\|\rightarrow\infty\)</span>, we introduce the normalization condition <span class="math">\(\mathbf{u}_1^T\mathbf{u}_1=1\)</span> using Lagrange multiplier <span class="math">\(\lambda_1\)</span>.</p>
<p>Then, the object function is given by</p>
<div class="math">$$\begin{aligned}
L &amp;= \mathbf{u}_1^T\mathbf{Su}_1 + \lambda_1\left(1-\mathbf{u}_1^T\mathbf{u}_1\right)\\
\frac{\partial L}{\partial \mathbf{u}_1} &amp;= 2\mathbf{S}\mathbf{u}_1 - 2\lambda_1\mathbf{u}_1 = 0\\
\therefore\mathbf{u}_1^T\mathbf{Su}_1 &amp;= \lambda_1 \quad (\because \text{left-multiplied $\mathbf{u}_1^T$})
\end{aligned}$$</div>
<p>The variance will be a maximum when we set <span class="math">\(\mathbf{u}_1\)</span> eaual to the eigenvector having the largest eigenvalue <span class="math">\(\lambda_1\)</span>. This eigenvector is known as the first principal component.</p>
<p>We can define additional principal components in the same way.</p>
<div class="admonition summary">
  <p class="admonition-title">Summary</p>
  <p>If we consider an M-dimensional projection space, the optimal linear projection is defined by the M eigenvectors u1,...,uM of the data covariance matrix S corresponding to the M largest eigenvalues λ1,...,λM.</p>
</div>

<h2>Kernel PCA</h2>
<div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">kerasy</span><span class="o">.</span><span class="n">ML</span><span class="o">.</span><span class="n">decomposition</span><span class="o">.</span><span class="n">KernelPCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="s2">&quot;gaussian&quot;</span><span class="p">,</span> <span class="o">**</span><span class="n">kernelargs</span><span class="p">)</span>
</pre></div>


<p>The main idea is same with PCA, but we will perform it <strong>in the feature space</strong>, which implicitly defines a nonlinear principal component model in the original data space.</p>
<p>If we <font color="red"><b>assume that the projected data set has zero mean</b></font>, The covariance matrix in feature space is given by</p>
<div class="math">$$
\mathbf{C} = \frac{1}{N}\sum_{n=1}^N\phi(\mathbf{x}_n)\phi(\mathbf{x}_n)^T
$$</div>
<p>and its <strong>eigenvector expansion</strong> is defined by</p>
<div class="math">$$\begin{aligned}
\mathbf{C}\mathbf{v}_i &amp;= \lambda_i\mathbf{v}_i\\
\frac{1}{N}\sum_{n=1}^N\phi(\mathbf{x}_n)\underset{\text{scalar}}{\left\{\phi(\mathbf{x}_n)^T\mathbf{v}_i\right\}} &amp;= \lambda_i\mathbf{v}_i\quad (\ast)\\
\end{aligned}$$</div>
<p>From this equation, we see that the vector <span class="math">\(\mathbf{v}_i\)</span> is given by a linear combination of the <span class="math">\(\phi(\mathbf{x}_n)\)</span></p>
<div class="math">$$\mathbf{v}_i = \sum_{n=1}^N a_{in}\phi(\mathbf{x}_n)$$</div>
<p>Substituting this expansion back into the eigenvector equation <span class="math">\((\ast)\)</span>, we obtain</p>
<div class="math">$$\begin{aligned}
\frac{1}{N}\sum_{n=1}^N\phi(\mathbf{x_n})\phi(\mathbf{x}_n)^T\sum_{m=1}^Na_{im}\phi(\mathbf{x}_m) &amp;= \lambda_i\sum_{n=1}^Na_{in}\phi(\mathbf{x}_n)\\
\frac{1}{N}\sum_{n=1}^Nk(\mathbf{x}_l,\mathbf{x}_n)\sum_{m=1}^Na_{i,m}k(\mathbf{x}_n,\mathbf{x}_m) &amp;= \lambda_i\sum_{n=1}^Na_{in}k(\mathbf{x}_l,\mathbf{x}_n)\quad(\because\text{multiplied $\phi(\mathbf{x}_l)^T$})\\
\mathbf{K}^2\mathbf{a}_i &amp;= \lambda_iN\mathbf{Ka}_i\quad (\because\text{written in matrix notation})\\
\end{aligned}$$</div>
<p>We can find solutions for <span class="math">\(\mathbf{a}_i\)</span> by solving the following eigenvalue problem. The variance in feature space will be a maximum when we set <span class="math">\(\mathbf{a}_1\)</span> eaual to the eigenvector having the largest eigenvalue <span class="math">\(\lambda_i\)</span>.</p>
<div class="math">$$\mathbf{Ka}_i = \lambda_iN\mathbf{a}_i$$</div>
<p>Having solved the eigenvector problem, the projection of a point <span class="math">\(\mathbf{x}\)</span> onto eigenvector <span class="math">\(i\)</span> is given by</p>
<div class="math">$$y_i(\mathbf{x}) = \phi(\mathbf{x})^T\mathbf{v}_i = \sum_{n=1}^Na_{in}\phi(\mathbf{x})^T\phi(\mathbf{x}_n) = \sum_{n=1}^Na_{in}k(\mathbf{x},\mathbf{x}_n)$$</div>
<p>If <font color="red"><b>projected data set doesn't have zero mean</b></font>, the projected data points after centralizing are given by</p>
<div class="math">$$\tilde{\phi(\mathbf{x}_n)} = \phi(\mathbf{x}_n)-\frac{1}{N}\sum_{l=1}^N\phi(\mathbf{x}_l)$$</div>
<p>and the corresponding elements of the Gram matrix are given by</p>
<div class="math">$$\tilde{\mathbf{K}} = \mathbf{K} - \mathbf{1_NK} - \mathbf{K1_N} +\mathbf{1_NK1_N}$$</div>
<p>where <span class="math">\(\mathbf{1}_N\)</span> denotes the <span class="math">\(N\times N\)</span> matrix in which every element takes the value <span class="math">\(1/N\)</span>.</p>
<div class="admonition warning">
  <p class="admonition-title">Warning</p>
  <p>I don't understand clearly how to deal with new data point xn.</p>
</div>

<h2>SNE</h2>
<p><font color="red"><b>Stochastic Neighbor Embedding (SNE)</b></font> starts by <b>converting the high-dimensional Euclidean distances between datapoints into conditional probabilities</b>. The similarity of datapoint <span class="math">\(\mathbf{x}_j\)</span> to datapoint <span class="math">\(\mathbf{x}_i\)</span> is conditional probability <span class="math">\(p_{j|i}\)</span>, that <span class="math">\(\mathbf{x}_i\)</span> would pick <span class="math">\(\mathbf{x}_j\)</span> as its neighbor <b>if neighbors were picked in propotion to their probability density under <font color="red">Gaussian centered at <span class="math">\(\mathbf{x}_i\)</span></font></b>.</p>
<p>Mathmatically, the conditional probability <span class="math">\(p_{j|i}\)</span> is given by</p>
<div class="math">$$p_{j|i} = \frac{\exp\left(-\|\mathbf{x}_i-\mathbf{x}_j\|^2/2\boldsymbol{\sigma}_i^2\right)}{\sum_{k\neq i}\exp\left(-\|\mathbf{x}_i-\mathbf{x}_k\|^2/2\boldsymbol{\sigma}_i^2\right)}$$</div>
<p>Because we are only interested in modeling pairwise similarities, we set the value of <span class="math">\(p_{i|i}\)</span> to zero.</p>
<p>For the low-dimensional counterparts <span class="math">\(\mathbf{y}_i\)</span> and <span class="math">\(\mathbf{y}_j\)</span> of the high-dimensional data points <span class="math">\(\mathbf{x}_i\)</span> and <span class="math">\(\mathbf{x}_j\)</span>, it is possible to compute a similar conditional probability, which we denote by <span class="math">\(q_{j|i}\)</span></p>
<div class="math">$$q_{j|i} = \frac{\exp\left(-\|\mathbf{y}_i-\mathbf{y}_j\|^2\right)}{\sum_{k\neq i}\exp\left(-\|\mathbf{y}_i-\mathbf{y}_k\|^2\right)}$$</div>
<p>Again, we set <span class="math">\(q_{i|i} = 0\)</span>, and the variance of the Gaussian to <span class="math">\(\frac{1}{\sqrt{2}}\)</span></p>
<p>Then, the cost function is given by</p>
<div class="math">$$C = \sum_i\mathrm{KL}(P_i\|Q_i) = \sum_i\sum_j p_{j|i}\log\frac{p_{j|i}}{q_{j|i}}$$</div>
<p>in which <span class="math">\(P_i\)</span> represents <b>the conditional probability distribution over all other data points given data points <span class="math">\(\mathbf{x}_i\)</span></b></p>
<p>The remaining parameter to be selected is the variance <span class="math">\(\boldsymbol{\sigma_i}\)</span>. It is defined to produce a <span class="math">\(P_i\)</span> with a fixed <font color="red"><b>perplexity</b></font> that is specified by the user. The perplexity is defined as</p>
<div class="math">$$Perp(P_i) = 2^{H(P_i)},\quad H(P_i) = -\sum_jp_{j|i}\log_2p_{j|i}$$</div>
<p>The minimization of the cost function is performed using a gradient descent method. The gradient has a surprisingly simple form</p>
<div class="math">$$\frac{\partial C}{\partial \mathbf{y}_i} = 2\sum_j\left(p_{j|i} - q_{j|i} + p_{i|j} - q_{i|j}\right)\left(\mathbf{y}_i-\mathbf{y}_j\right)$$</div>
<h2>tSNE</h2>
<div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">kerasy</span><span class="o">.</span><span class="n">ML</span><span class="o">.</span><span class="n">decomposition</span><span class="o">.</span><span class="n">tSNE</span><span class="p">(</span><span class="n">initial_momentum</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">final_momoentum</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">eta</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">min_gain</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">prec_max_iter</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
</pre></div>


<p>As an alternative to minimizing the sum of the Kullback-Leibler divergence between a joint probability distribution <span class="math">\(p_{j|i}\)</span> and <span class="math">\(q_{j|i}\)</span>, it is also possible to minimize a single Kullback-Leibler divergence between a joint probability distribution:</p>
<div class="math">$$C = KL\left(P\|Q\right) = \sum_i\sum_jp_{ij}\log\frac{p_{ij}}{q_{ij}}$$</div>
<p>In t-SNE,
- Employ a <b>Student t-distribution with one degree of freedom (which is the same as a Cauchy distribution)</b> as the heavy-tailed distribution in the <b>low-dimensional map.</b> Using this distribution, the joint probabilities <span class="math">\(q_{ij}\)</span> are defined as
</p>
<div class="math">$$q_{ij} = \frac{\left(1 + \|\mathbf{y}_i-\mathbf{y}_j\|^2\right)^{-1}}{\sum_{k\neq l}\left(1 + \|\mathbf{y}_k-\mathbf{y}_l\|^2\right)^{-1}}$$</div>
<p>
- If there is an outlier <span class="math">\(\mathbf{x}_i\)</span>, low-dimensional map point <span class="math">\(\mathbf{y}_i\)</span> has very little effect on the cost function, so set
</p>
<div class="math">$$p_{ij} = \frac{p_{j|i} + p_{i|j}}{2n}$$</div>
<p>
to ensure that <span class="math">\(\sum_jp_{ij}&gt;\frac{1}{2n}\)</span> for all datapoints <span class="math">\(\mathbf{x}_i\)</span></p>
<p>The gradient is given by</p>
<div class="math">$$\frac{\partial C}{\partial \mathbf{y}_i} = 4\sum_j\left(p_{ij} - q_{ij}\right) \left(\mathbf{y}_i - \mathbf{y}_j\right)\left(1 + \|\mathbf{y}_i - \mathbf{y}_j\|^2\right)^{-1}$$</div>
<div class="admonition reference">
  <p class="admonition-title">Reference</p>
  <ul>
    <li><a href="https://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf">Visualizing Data using t-SNE</a></li>
    <li><a href="https://www.microsoft.com/en-us/research/people/cmbishop/#!prml-book">Pattern Recognition and Machine Learning by Christopher Bishop</a></li>
  </ul>
</div>

<h2>UMAP</h2>
<div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">kerasy</span><span class="o">.</span><span class="n">ML</span><span class="o">.</span><span class="n">decomposition</span><span class="o">.</span><span class="n">UMAP</span><span class="p">(</span><span class="n">metric</span><span class="o">=</span><span class="s2">&quot;euclidean&quot;</span><span class="p">,</span> <span class="n">metric_kwds</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">min_dist</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">b</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sigma_iter</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">sigma_tol</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">sigma_lower</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma_upper</span><span class="o">=</span><span class="mf">1e3</span><span class="p">)</span>
</pre></div>


<p>UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology.</p>
<div class="admonition reference">
  <p class="admonition-title">Reference</p>
  <ul>
    <li><a href="https://arxiv.org/pdf/1802.03426.pdf">UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction</a></li>
  </ul>
</div>

<h4>tSNE vs. UMAP</h4>
<table>
<thead>
<tr>
<th align="center"></th>
<th align="center">tSNE</th>
<th align="center">UMAP</th>
</tr>
</thead>
<tbody>
<tr>
<td align="center"></td>
<td align="center"><strong>pure Machine Learning semi-empirical algorithm</strong></td>
<td align="center"><strong>based on solid mathematical principles</strong></td>
</tr>
<tr>
<td align="center"></td>
<td align="center">Preserves <strong>Only Local</strong> Structure</td>
<td align="center">Preserves <strong>Global</strong> Structure</td>
</tr>
<tr>
<td align="center">probability in <strong>high-dimensional space</strong></td>
<td align="center">
<div class="math">$$p_{j\mid i} = \frac{\exp\left(-\|x_i-x_j\|^2/2\sigma_i^2\right)}{\sum_{k\neq i}\exp\left(-\|x_i-x_k\|^2/2\sigma_i^2\right)}$$</div>
</td>
<td align="center">
<div class="math">$$p_{i\mid j} = \exp\left(-\frac{d(x_i,x_j)-\rho_i}{\sigma_i}\right), \quad \rho_i = \min_{j\neq i}\left\{d(x_i,x_j)\right\}$$</div>
</td>
</tr>
<tr>
<td align="center">joint probability in <strong>high-dimensional space</strong></td>
<td align="center">
<div class="math">$$p_{ij}=\frac{p_{i\mid j}+p_{j\mid i}}{2N}$$</div>
</td>
<td align="center">
<div class="math">$$p_{ij} = p_{i\mid j} + p_{j\mid i} - p_{i\mid j}p_{j\mid i}$$</div>
</td>
</tr>
<tr>
<td align="center"></td>
<td align="center">
<div class="math">$$\text{Perplexity} = 2^{-\sum_j p_{j\mid i}\log_2p_{j\mid i}}$$</div>
</td>
<td align="center"><strong>number of nearest neighbors</strong><div class="math">$$k = 2^{\sum_jp_{i\mid j}}$$</div>
</td>
</tr>
<tr>
<td align="center">probability in <strong>low-dimensional space</strong></td>
<td align="center">
<div class="math">$$q_{ij}=\frac{\left(1 + \|y_i-y_j\|^2\right)^{-1}}{\sum_{k\neq i}\left(1 + \|y_i-y_k\|^2\right)^{-1}}$$</div>
</td>
<td align="center">
<div class="math">$$q_{ij} = \left(1 + a(y_i-y_j)^{2b}\right)^{-1}$$</div>
</td>
</tr>
<tr>
<td align="center">loss function</td>
<td align="center"><span class="math">\(\mathrm{KL}(P_i\|Q_i) = \sum_i\sum_jp_{j\mid i}\log\frac{p_{j\mid i}}{q_{j\mid i}}\)</span></td>
<td align="center">
<div class="math">$$\mathrm{CE}(X,Y) = \sum_i\sum_j\left[p_{ij}(X)\log\left(\frac{p_{ij}(X)}{q_{ij}(Y)}\right) + \left(1-p_{ij}(X)\right)\log\left(\frac{1-p_{ij}(X)}{1-q_{ij}(Y)}\right)\right]$$</div>
</td>
</tr>
<tr>
<td align="center">Optimization</td>
<td align="center"><strong>Gradient Descent</strong></td>
<td align="center"><strong>Stochastic Gradient Descent</strong></td>
</tr>
<tr>
<td align="center">Gradients</td>
<td align="center">
<div class="math">$$\frac{\partial\mathrm{KL}}{\partial y_i} = 4\sum_j(p_{ij}-q_{ij})(y_i-y_j)\left(1 + \|y_i-y_j\|^2\right)^{-1}$$</div>
</td>
<td align="center">
<div class="math">$$\frac{\partial\mathrm{CE}}{\partial y_i} = \sum_j\left[\frac{2abd_{ij}^{2(b-1)}P(X)}{1 + ad_{ij}^{2b}}-\frac{2b(1-P(X))}{d_{ij}^2\left(1 + ad_{ij}^{2b}\right)}\right](y_i-y_j)$$</div>
</td>
</tr>
<tr>
<td align="center">Initial low-dimensional coordinates</td>
<td align="center"><strong>Random Normal Initialization</strong></td>
<td align="center"><strong>Graph Laplacian</strong></td>
</tr>
</tbody>
</table>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        fonts: [['STIX', 'TeX']]," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
        </section>
    </div>
</div>
</div>
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../sampling/" class="btn btn-neutral float-right" title="Sampling">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../../DeepLearning/Initializers/" class="btn btn-neutral" title="Initializers"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../../DeepLearning/Initializers/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../sampling/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '../..';</script>
    <script src="../../js/theme.js" defer></script>
      <script src="../../search/main.js" defer></script>

</body>
</html>
